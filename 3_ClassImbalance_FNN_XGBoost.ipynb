{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "342647ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of POW features: 70\n",
      "Label distribution: [14461 54370]\n",
      "Train size: (58110, 70) Test size: (10721, 70)\n",
      "Train label distribution: [12072 46038]\n",
      "Test label distribution: [2389 8332]\n",
      "\n",
      "========== FNN (MLPClassifier) - baseline ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\eeg-based-learning\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7344\n",
      "F1-score: 0.8360\n",
      "ROC-AUC:  0.7037\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.26      0.30      2389\n",
      "           1       0.80      0.87      0.84      8332\n",
      "\n",
      "    accuracy                           0.73     10721\n",
      "   macro avg       0.58      0.56      0.57     10721\n",
      "weighted avg       0.71      0.73      0.72     10721\n",
      "\n",
      "\n",
      "========== FNN (MLPClassifier) - balanced_only (sample_weight) ==========\n",
      "w_neg (class 0): 0.7922560660815694 w_pos (class 1): 0.20774393391843057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\eeg-based-learning\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7317\n",
      "F1-score: 0.8379\n",
      "ROC-AUC:  0.6774\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.17      0.22      2389\n",
      "           1       0.79      0.89      0.84      8332\n",
      "\n",
      "    accuracy                           0.73     10721\n",
      "   macro avg       0.55      0.53      0.53     10721\n",
      "weighted avg       0.68      0.73      0.70     10721\n",
      "\n",
      "\n",
      "========== FNN (MLPClassifier) - smote_only ==========\n",
      "After SMOTE train distribution: [46038 46038]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\eeg-based-learning\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7178\n",
      "F1-score: 0.8320\n",
      "ROC-AUC:  0.6267\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.08      0.12      2389\n",
      "           1       0.77      0.90      0.83      8332\n",
      "\n",
      "    accuracy                           0.72     10721\n",
      "   macro avg       0.48      0.49      0.47     10721\n",
      "weighted avg       0.64      0.72      0.67     10721\n",
      "\n",
      "\n",
      "========== XGBoost - baseline ==========\n",
      "Accuracy: 0.7629\n",
      "F1-score: 0.8651\n",
      "ROC-AUC:  0.4601\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.01      0.02      2389\n",
      "           1       0.78      0.98      0.87      8332\n",
      "\n",
      "    accuracy                           0.76     10721\n",
      "   macro avg       0.45      0.49      0.44     10721\n",
      "weighted avg       0.63      0.76      0.68     10721\n",
      "\n",
      "\n",
      "========== XGBoost - balanced_only (scale_pos_weight) ==========\n",
      "scale_pos_weight: 0.2622181676006777\n",
      "Accuracy: 0.5771\n",
      "F1-score: 0.7097\n",
      "ROC-AUC:  0.4448\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.27      0.22      2389\n",
      "           1       0.76      0.67      0.71      8332\n",
      "\n",
      "    accuracy                           0.58     10721\n",
      "   macro avg       0.47      0.47      0.47     10721\n",
      "weighted avg       0.63      0.58      0.60     10721\n",
      "\n",
      "\n",
      "========== XGBoost - smote_only ==========\n",
      "After SMOTE train distribution: [46038 46038]\n",
      "Accuracy: 0.6093\n",
      "F1-score: 0.7328\n",
      "ROC-AUC:  0.4890\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.33      0.27      2389\n",
      "           1       0.78      0.69      0.73      8332\n",
      "\n",
      "    accuracy                           0.61     10721\n",
      "   macro avg       0.51      0.51      0.50     10721\n",
      "weighted avg       0.66      0.61      0.63     10721\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>variant</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FNN</td>\n",
       "      <td>balanced_only</td>\n",
       "      <td>0.731741</td>\n",
       "      <td>0.837935</td>\n",
       "      <td>0.677431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FNN</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.734446</td>\n",
       "      <td>0.836031</td>\n",
       "      <td>0.703672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FNN</td>\n",
       "      <td>smote_only</td>\n",
       "      <td>0.717750</td>\n",
       "      <td>0.832001</td>\n",
       "      <td>0.626722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>balanced_only</td>\n",
       "      <td>0.577092</td>\n",
       "      <td>0.709694</td>\n",
       "      <td>0.444811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.762895</td>\n",
       "      <td>0.865103</td>\n",
       "      <td>0.460108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>smote_only</td>\n",
       "      <td>0.609272</td>\n",
       "      <td>0.732759</td>\n",
       "      <td>0.489045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model        variant  accuracy        f1       auc\n",
       "1      FNN  balanced_only  0.731741  0.837935  0.677431\n",
       "0      FNN       baseline  0.734446  0.836031  0.703672\n",
       "2      FNN     smote_only  0.717750  0.832001  0.626722\n",
       "4  XGBoost  balanced_only  0.577092  0.709694  0.444811\n",
       "3  XGBoost       baseline  0.762895  0.865103  0.460108\n",
       "5  XGBoost     smote_only  0.609272  0.732759  0.489045"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3_ClassImbalance_FNN_XGBoost.ipynb\n",
    "# Experiments with:\n",
    "# - only POW.* features\n",
    "# - baseline vs class_weight='balanced' vs SMOTE\n",
    "# - FNN (Keras) and XGBoost\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. LOAD DATA \n",
    "# -------------------------------------------------------\n",
    "\n",
    "raw = pd.read_csv(\"data/raw/EEG_data.csv\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. SELECT FEATURES: ONLY POW.* + LABEL + GROUP\n",
    "# -------------------------------------------------------\n",
    "\n",
    "subjects = raw[\"subject_id\"]\n",
    "y = raw[\"subject_understood\"].astype(int)\n",
    "pow_cols = [c for c in raw.columns if c.startswith(\"POW\")]\n",
    "X = raw[pow_cols]\n",
    "\n",
    "print(\"Number of POW features:\", len(pow_cols))\n",
    "print(\"Label distribution:\", np.bincount(y))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. SUBJECT-WISE TRAIN / TEST SPLIT\n",
    "# -------------------------------------------------------\n",
    "\n",
    "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=RANDOM_STATE)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=subjects))\n",
    "\n",
    "X_train_raw, X_test_raw = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "print(\"Train size:\", X_train_raw.shape, \"Test size:\", X_test_raw.shape)\n",
    "print(\"Train label distribution:\", np.bincount(y_train))\n",
    "print(\"Test label distribution:\", np.bincount(y_test))\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. HELPERS: SCALING, CLASS WEIGHTS, FNN BUILDER, RUNNER\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def scale_data(X_train_raw, X_test_raw):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "    X_test_scaled = scaler.transform(X_test_raw)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "def compute_class_weights_vector(y_train):\n",
    "    \"\"\"Tak samo jak w Notebooku 2: wagi w zależności od klasy.\"\"\"\n",
    "    pos = (y_train == 1).sum()\n",
    "    neg = (y_train == 0).sum()\n",
    "    w_pos = neg / (pos + neg)\n",
    "    w_neg = pos / (pos + neg)\n",
    "    sample_weight = np.where(y_train == 1, w_pos, w_neg)\n",
    "    return sample_weight, (w_neg, w_pos)\n",
    "\n",
    "def evaluate_binary(y_true, y_pred, y_prob=None, verbose=True):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_prob)\n",
    "        except ValueError:\n",
    "            auc = np.nan\n",
    "    else:\n",
    "        auc = np.nan\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(f\"F1-score: {f1:.4f}\")\n",
    "        print(f\"ROC-AUC:  {auc:.4f}\")\n",
    "        print(\"\\nClassification report:\")\n",
    "        print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"auc\": auc}\n",
    "\n",
    "results = []\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5. Define experiment variants (FNN = MLPClassifier)\n",
    "#    - baseline: no reweighting, no SMOTE\n",
    "#    - balanced: class_weight / scale_pos_weight\n",
    "#    - smote: SMOTE on training (roughly half-half)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def make_fnn():\n",
    "    return MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=1e-4,\n",
    "        batch_size=256,\n",
    "        learning_rate_init=1e-3,\n",
    "        max_iter=50,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "# 5a) baseline: bez wag, bez SMOTE\n",
    "print(\"\\n========== FNN (MLPClassifier) - baseline ==========\")\n",
    "X_train_scaled, X_test_scaled = scale_data(X_train_raw, X_test_raw)\n",
    "\n",
    "fnn_baseline = make_fnn()\n",
    "fnn_baseline.fit(X_train_scaled, y_train)\n",
    "y_pred = fnn_baseline.predict(X_test_scaled)\n",
    "y_prob = fnn_baseline.predict_proba(X_test_scaled)[:, 1]\n",
    "metrics = evaluate_binary(y_test, y_pred, y_prob)\n",
    "\n",
    "results.append({\"model\": \"FNN\", \"variant\": \"baseline\", **metrics})\n",
    "\n",
    "# 5b) balanced_only: sample_weight jak w Notebooku 2\n",
    "print(\"\\n========== FNN (MLPClassifier) - balanced_only (sample_weight) ==========\")\n",
    "X_train_scaled, X_test_scaled = scale_data(X_train_raw, X_test_raw)\n",
    "sample_weight, (w_neg, w_pos) = compute_class_weights_vector(y_train)\n",
    "print(\"w_neg (class 0):\", w_neg, \"w_pos (class 1):\", w_pos)\n",
    "\n",
    "fnn_balanced = make_fnn()\n",
    "fnn_balanced.fit(X_train_scaled, y_train, sample_weight=sample_weight)\n",
    "y_pred = fnn_balanced.predict(X_test_scaled)\n",
    "y_prob = fnn_balanced.predict_proba(X_test_scaled)[:, 1]\n",
    "metrics = evaluate_binary(y_test, y_pred, y_prob)\n",
    "\n",
    "results.append({\"model\": \"FNN\", \"variant\": \"balanced_only\", **metrics})\n",
    "\n",
    "# 5c) smote_only: SMOTE na train, bez dodatkowych wag\n",
    "print(\"\\n========== FNN (MLPClassifier) - smote_only ==========\")\n",
    "sm = SMOTE(sampling_strategy=\"auto\", random_state=RANDOM_STATE)\n",
    "X_train_sm_raw, y_train_sm = sm.fit_resample(X_train_raw, y_train)\n",
    "print(\"After SMOTE train distribution:\", np.bincount(y_train_sm))\n",
    "\n",
    "X_train_scaled_sm, X_test_scaled = scale_data(X_train_sm_raw, X_test_raw)\n",
    "\n",
    "fnn_smote = make_fnn()\n",
    "fnn_smote.fit(X_train_scaled_sm, y_train_sm)\n",
    "y_pred = fnn_smote.predict(X_test_scaled)\n",
    "y_prob = fnn_smote.predict_proba(X_test_scaled)[:, 1]\n",
    "metrics = evaluate_binary(y_test, y_pred, y_prob)\n",
    "\n",
    "results.append({\"model\": \"FNN\", \"variant\": \"smote_only\", **metrics})\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6. XGBoost – three variants\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def make_xgb(scale_pos_weight=1.0):\n",
    "    return xgb.XGBClassifier(\n",
    "        n_estimators=5,           # tak jak w Notebooku 2\n",
    "        learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        tree_method=\"hist\"\n",
    "    )\n",
    "\n",
    "# 6a) baseline: bez wag, bez SMOTE\n",
    "print(\"\\n========== XGBoost - baseline ==========\")\n",
    "X_train_scaled, X_test_scaled = scale_data(X_train_raw, X_test_raw)\n",
    "\n",
    "xgb_baseline = make_xgb(scale_pos_weight=1.0)\n",
    "xgb_baseline.fit(X_train_scaled, y_train)\n",
    "y_pred = xgb_baseline.predict(X_test_scaled)\n",
    "y_prob = xgb_baseline.predict_proba(X_test_scaled)[:, 1]\n",
    "metrics = evaluate_binary(y_test, y_pred, y_prob)\n",
    "\n",
    "results.append({\"model\": \"XGBoost\", \"variant\": \"baseline\", **metrics})\n",
    "\n",
    "# 6b) balanced_only: scale_pos_weight zgodnie z proporcjami\n",
    "print(\"\\n========== XGBoost - balanced_only (scale_pos_weight) ==========\")\n",
    "pos = (y_train == 1).sum()\n",
    "neg = (y_train == 0).sum()\n",
    "scale_pos_weight = neg / max(pos, 1)\n",
    "print(\"scale_pos_weight:\", scale_pos_weight)\n",
    "\n",
    "X_train_scaled, X_test_scaled = scale_data(X_train_raw, X_test_raw)\n",
    "\n",
    "xgb_balanced = make_xgb(scale_pos_weight=scale_pos_weight)\n",
    "xgb_balanced.fit(X_train_scaled, y_train)\n",
    "y_pred = xgb_balanced.predict(X_test_scaled)\n",
    "y_prob = xgb_balanced.predict_proba(X_test_scaled)[:, 1]\n",
    "metrics = evaluate_binary(y_test, y_pred, y_prob)\n",
    "\n",
    "results.append({\"model\": \"XGBoost\", \"variant\": \"balanced_only\", **metrics})\n",
    "\n",
    "# 6c) smote_only: SMOTE na train, scale_pos_weight = 1\n",
    "print(\"\\n========== XGBoost - smote_only ==========\")\n",
    "sm = SMOTE(sampling_strategy=\"auto\", random_state=RANDOM_STATE)\n",
    "X_train_sm_raw, y_train_sm = sm.fit_resample(X_train_raw, y_train)\n",
    "print(\"After SMOTE train distribution:\", np.bincount(y_train_sm))\n",
    "\n",
    "X_train_scaled_sm, X_test_scaled = scale_data(X_train_sm_raw, X_test_raw)\n",
    "\n",
    "xgb_smote = make_xgb(scale_pos_weight=1.0)\n",
    "xgb_smote.fit(X_train_scaled_sm, y_train_sm)\n",
    "y_pred = xgb_smote.predict(X_test_scaled)\n",
    "y_prob = xgb_smote.predict_proba(X_test_scaled)[:, 1]\n",
    "metrics = evaluate_binary(y_test, y_pred, y_prob)\n",
    "\n",
    "results.append({\"model\": \"XGBoost\", \"variant\": \"smote_only\", **metrics})\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7. SUMMARY TABLE\n",
    "# -------------------------------------------------------\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values([\"model\", \"variant\"])\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
